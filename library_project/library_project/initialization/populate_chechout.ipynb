{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 1000 patron accounts\n",
      "Sample of selected books:\n",
      "[(96, 25)]\n"
     ]
    }
   ],
   "source": [
    "fake = Faker()\n",
    "n_patrons = 1000\n",
    "print(f\"Building {n_patrons} patron accounts\")\n",
    "values = tuple(\n",
    "    zip([fake.name() for _ in range(n_patrons)],\n",
    "        [fake.address() for _ in range(n_patrons)],\n",
    "        [fake.unique.email() for _ in range(n_patrons)]))\n",
    "\n",
    "## CHAT GPT SUGGESTED ##\n",
    "\n",
    "# Function to simulate book selection\n",
    "# Function to simulate book selection\n",
    "def simulate_book_selection(persons, library):\n",
    "    selections = []\n",
    "    for person in persons:\n",
    "        if np.random.rand() < person['probability_of_reading']:\n",
    "            # Calculate weights based on hidden preferences\n",
    "            weights = np.array([person['hidden_preferences'].get(genre, 0) for genre in library['Genre']])\n",
    "\n",
    "            # Normalize weights to make them probabilities\n",
    "            weights /= weights.sum()\n",
    "\n",
    "            selected_book = np.random.choice(library['BookID'], p=weights)\n",
    "            selections.append((person['person_id'], selected_book))\n",
    "    return selections\n",
    "\n",
    "# Generate a list of book genres\n",
    "book_genres = ['Fiction', 'Mystery', 'Sci-Fi', 'Romance', 'Non-Fiction', 'Fantasy']\n",
    "\n",
    "# Create a library of books with genres\n",
    "num_books = 1000\n",
    "library = pd.DataFrame({\n",
    "    'BookID': np.arange(1, num_books + 1),\n",
    "    'Genre': np.random.choice(book_genres, size=num_books)\n",
    "})\n",
    "\n",
    "# Create a list of persons with assigned genres and hidden preferences\n",
    "num_persons = 100\n",
    "persons = []\n",
    "for person_id in range(1, num_persons + 1):\n",
    "    assigned_genres = np.random.choice(book_genres, size=np.random.randint(1, len(book_genres) + 1), replace=False)\n",
    "\n",
    "    # Generate hidden preferences for each person\n",
    "    hidden_preferences = {genre: np.random.rand() for genre in book_genres}\n",
    "\n",
    "    probability_of_reading = 0.01  # Very low probability of reading for each person\n",
    "    persons.append({\n",
    "        'person_id': person_id,\n",
    "        'assigned_genres': assigned_genres,\n",
    "        'hidden_preferences': hidden_preferences,\n",
    "        'probability_of_reading': probability_of_reading\n",
    "    })\n",
    "\n",
    "# Simulate book selections for 100,000 iterations\n",
    "num_iterations = 100000\n",
    "selected_books = simulate_book_selection(persons, library)\n",
    "\n",
    "# Display a sample of the selected books\n",
    "print(\"Sample of selected books:\")\n",
    "print(selected_books[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.4583333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Morri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Morri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text and remove stop words\n",
    "    tokens = [word.lower() for word in word_tokenize(text) if word.isalnum() and word.lower() not in stopwords.words('english')]\n",
    "    return set(tokens)\n",
    "\n",
    "def weighted_jaccard_similarity(book1, book2):\n",
    "    # Define weights for each text field\n",
    "    weights = {'title': 3, 'authors': 2, 'publisher': 1.5, 'description': 1, 'genres': 1}\n",
    "\n",
    "    # Preprocess each text field\n",
    "    preprocessed_fields1 = {field: preprocess_text(book1[field]) for field in weights.keys()}\n",
    "    preprocessed_fields2 = {field: preprocess_text(book2[field]) for field in weights.keys()}\n",
    "\n",
    "    # Calculate weighted Jaccard similarity for each text field\n",
    "    similarities = [(field, weights[field] * len(preprocessed_fields1[field].intersection(preprocessed_fields2[field])) /\n",
    "                     len(preprocessed_fields1[field].union(preprocessed_fields2[field])))\n",
    "                    for field in weights.keys()]\n",
    "\n",
    "    # Combine individual similarities using the assigned weights\n",
    "    overall_similarity = sum(similarity for _, similarity in similarities)\n",
    "\n",
    "    return overall_similarity\n",
    "\n",
    "# Example usage:\n",
    "book1 = {\n",
    "    'title': 'The Catcher in the Rye',\n",
    "    'authors': 'J.D. Salinger',\n",
    "    'publisher': 'Little, Brown and Company',\n",
    "    'publish_year': 1951,\n",
    "    'description': 'A classic novel about a teenage boy',\n",
    "    'genres': 'Fiction, Coming-of-age'\n",
    "}\n",
    "\n",
    "book2 = {\n",
    "    'title': 'To Kill a Mockingbird',\n",
    "    'authors': 'Harper Lee',\n",
    "    'publisher': 'J.B. Lippincott & Co.',\n",
    "    'publish_year': 1960,\n",
    "    'description': 'A novel set in the American South during the 1930s',\n",
    "    'genres': 'Fiction, Southern Gothic'\n",
    "}\n",
    "\n",
    "similarity_score = weighted_jaccard_similarity(book1, book2)\n",
    "print(f\"Similarity Score: {similarity_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating bookshelves...\n",
      "Generated 363 bookshelves containing 40000 books\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word',\n",
    "                      ngram_range=(1, 10),\n",
    "                      stop_words = 'english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = tfidf.fit_transform(books[\"Title\"].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<26737x341661 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 458900 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.33 GiB for an array with shape (26737, 26737) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\library_project\\library_project\\initialization\\populate_chechout.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/dbsys/library-project/library_project/library_project/initialization/populate_chechout.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m similarity_matrix \u001b[39m=\u001b[39m cosine_similarity(vecs, vecs)\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    215\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1586\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1583\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1584\u001b[0m     Y_normalized \u001b[39m=\u001b[39m normalize(Y, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m-> 1586\u001b[0m K \u001b[39m=\u001b[39m safe_sparse_dot(X_normalized, Y_normalized\u001b[39m.\u001b[39;49mT, dense_output\u001b[39m=\u001b[39;49mdense_output)\n\u001b[0;32m   1588\u001b[0m \u001b[39mreturn\u001b[39;00m K\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\.venv\\Lib\\site-packages\\sklearn\\utils\\extmath.py:200\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    192\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39m b\n\u001b[0;32m    194\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    195\u001b[0m     sparse\u001b[39m.\u001b[39missparse(a)\n\u001b[0;32m    196\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[0;32m    197\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[0;32m    198\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    199\u001b[0m ):\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39;49mtoarray()\n\u001b[0;32m    201\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\.venv\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1050\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m order \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1049\u001b[0m     order \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap(\u001b[39m'\u001b[39m\u001b[39mcf\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 1050\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_toarray_args(order, out)\n\u001b[0;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mc_contiguous \u001b[39mor\u001b[39;00m out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mf_contiguous):\n\u001b[0;32m   1052\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOutput array must be C or F contiguous\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\.venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:1267\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m   1266\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1267\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.33 GiB for an array with shape (26737, 26737) and data type float64"
     ]
    }
   ],
   "source": [
    "similarity_matrix = cosine_similarity(vecs, vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating bookshelves...\n",
      "Generated 361 bookshelves containing 40000 books\n"
     ]
    }
   ],
   "source": [
    "import populate_books\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "\n",
    "books = populate_books.read_books_data()\n",
    "\n",
    "titles =[TaggedDocument(doc, [i]) for i, doc in enumerate(preprocess_documents(books[\"Title\"].drop_duplicates()))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['nation', 'danc', 'religion', 'ident', 'cultur', 'differ', 'caribbean'], tags=[0]),\n",
       " TaggedDocument(words=['jaquith', 'famili', 'america'], tags=[1]),\n",
       " TaggedDocument(words=['megaton', 'gambl'], tags=[2]),\n",
       " TaggedDocument(words=['sleep', 'boi'], tags=[3]),\n",
       " TaggedDocument(words=['dinner', 'antoin'], tags=[4])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Doc2Vec(titles,\n",
    "                vector_size=1000,\n",
    "                window=5,\n",
    "                min_count=1,\n",
    "                workers=50,\n",
    "                dm=0,\n",
    "                epochs=1000) # PV-DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'most_similar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\library_project\\library_project\\initialization\\populate_chechout.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/dbsys/library-project/library_project/library_project/initialization/populate_chechout.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m Doc2Vec\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39md2v.model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/dbsys/library-project/library_project/library_project/initialization/populate_chechout.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m similar_doc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdv\u001b[39m.\u001b[39;49mvectors\u001b[39m.\u001b[39;49mmost_similar(\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/dbsys/library-project/library_project/library_project/initialization/populate_chechout.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(similar_doc[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'most_similar'"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description =[TaggedDocument(doc, [i]) for i, doc in enumerate(preprocess_documents(books.drop_duplicates(\"Title\")[\"description\"]))]\n",
    "model_description = Doc2Vec(description,\n",
    "                vector_size=1000,\n",
    "                window=5,\n",
    "                min_count=1,\n",
    "                workers=50,\n",
    "                dm=0,\n",
    "                epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_vecs = []\n",
    "for book in books[\"Title\"].drop_duplicates():\n",
    "    book_vecs.append(model.infer_vector(preprocess_documents([book])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(metric=&#x27;cosine&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(metric=&#x27;cosine&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(metric='cosine')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "book_vecs = np.array([model.dv[i[0]] for _, i in titles])\n",
    "\n",
    "book_vecs = np.array(book_vecs)\n",
    "neigh = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
    "neigh.fit(book_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../recommendation/neighbors.pkl']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "joblib.dump(neigh, \"../recommendation/neighbors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(book_vecs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = model.infer_vector(preprocess_documents([\"Bible Jesus king\"])[0])\n",
    "dist, idxs = neigh.kneighbors([sample],5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12375  8367 13853 13270  7001]]\n"
     ]
    }
   ],
   "source": [
    "print (idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King Jesus,\n",
      "['king', 'jesu']\n",
      "[12375]\n",
      "The Name of the King\n",
      "['king']\n",
      "[8367]\n",
      "The Bible Jesus Read\n",
      "['bibl', 'jesu', 'read']\n",
      "[13853]\n",
      "In the Name of Jesus\n",
      "['jesu']\n",
      "[13270]\n",
      "The King's Coat\n",
      "['king', 'coat']\n",
      "[7001]\n"
     ]
    }
   ],
   "source": [
    "no_dupe = books.drop_duplicates(\"Title\")\n",
    "for i in idxs[0]:\n",
    "    print(no_dupe.iloc[i][\"Title\"])\n",
    "    for title, tag in titles:\n",
    "        if i in tag:\n",
    "            print(title)\n",
    "            print(tag)\n",
    "    # print(titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "book_vec = []\n",
    "b1 = books.iloc[36591][\"Title\"]\n",
    "b2 = \"Researchers Delight\"\n",
    "b3 = \"Rugby and its implications\"\n",
    "book_vec.append(model.infer_vector(preprocess_documents([b1])[0]))\n",
    "book_vec.append(model.infer_vector(preprocess_documents([b2])[0]))\n",
    "book_vec.append(model.infer_vector(preprocess_documents([b3])[0]))\n",
    "book_vec = np.array(book_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_triangle_average(matrix):\n",
    "    lower_triangle = np.tril(matrix+3, -1)\n",
    "    lower_triangle_values = lower_triangle.flatten()\n",
    "    lower_triangle_values = lower_triangle[lower_triangle != 0]\n",
    "    average = np.mean(lower_triangle_values-3)\n",
    "    return average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21931298 0.23320505 0.28350484\n"
     ]
    }
   ],
   "source": [
    "book_vec_2 = []\n",
    "for book in books[books[\"categories\"] == \"['Health & Fitness']\"][\"Title\"]:\n",
    "    book_vec_2.append(model.infer_vector(preprocess_documents([book])[0]))\n",
    "book_vec_2 = np.array(book_vec_2)\n",
    "sim_mat_2 = cosine_similarity(book_vec_2, book_vec_2)\n",
    "print(lower_triangle_average(sim_mat_2),\n",
    "lower_triangle_average(np.abs(sim_mat_2)),\n",
    "np.sqrt(lower_triangle_average(np.square(sim_mat_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1497528 0.1691976 0.21097985\n"
     ]
    }
   ],
   "source": [
    "book_vec_3 = []\n",
    "for book in books[books[\"categories\"] == '[\"Misc\"]'][\"Title\"]:\n",
    "    book_vec_3.append(model.infer_vector(preprocess_documents([book])[0]))\n",
    "book_vec_3 = np.array(book_vec_3)\n",
    "sim_mat_3 = cosine_similarity(book_vec_3, book_vec_3)\n",
    "print(lower_triangle_average(sim_mat_3),\n",
    "lower_triangle_average(np.abs(sim_mat_3)),\n",
    "np.sqrt(lower_triangle_average(np.square(sim_mat_3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1685921"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_triangle_average(cosine_similarity(book_vec_2, book_vec_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000001 , 0.5533638 , 0.68679124],\n",
       "       [0.5533638 , 0.99999994, 0.1655393 ],\n",
       "       [0.68679124, 0.1655393 , 1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(book_vec, book_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8361    ['Health & Fitness']\n",
       "8362    ['Health & Fitness']\n",
       "8363    ['Health & Fitness']\n",
       "8364    ['Health & Fitness']\n",
       "8365    ['Health & Fitness']\n",
       "Name: categories, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[books[\"categories\"] != '[\"Misc\"]'][\"categories\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions =[TaggedDocument(doc, [i]) for i, doc in enumerate(preprocess_documents(books[\"description\"].drop_duplicates()))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36089    Situations: A Casebook of Virtual Realities fo...\n",
       "36090    Playing Along: 37 Group Learning Activities Bo...\n",
       "36091          Learning To Bow - Inside The Heart Of Japan\n",
       "36092    Boys Themselves: A Return to Single-Sex Education\n",
       "36093    The Missing 'Gator of Gumbo Limbo: An Ecologic...\n",
       "                               ...                        \n",
       "36591                               The Nature of Research\n",
       "36592    Grant Writing for Teachers: If You Can Write a...\n",
       "36593         Kids in Print: Publishing a School Newspaper\n",
       "36594    Cities of God And Nationalism Rome, Mecca, And...\n",
       "36595                                         Odd girl out\n",
       "Name: Title, Length: 507, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[books[\"categories\"]==\"['Education']\"][\"Title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist a model to disk with:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
