{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable builtin_function_or_method object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jayden\\Documents\\school\\database\\library_project\\library_project\\initialization\\populate_chechout.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jayden/Documents/school/database/library_project/library_project/initialization/populate_chechout.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m jin \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mnuts\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m'\u001b[39m\u001b[39msack\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mballs\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39m2 of them\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Jayden/Documents/school/database/library_project/library_project/initialization/populate_chechout.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m key,val \u001b[39min\u001b[39;00m jin\u001b[39m.\u001b[39mkeys,jin\u001b[39m.\u001b[39mvalues:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jayden/Documents/school/database/library_project/library_project/initialization/populate_chechout.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(key,val)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable builtin_function_or_method object"
     ]
    }
   ],
   "source": [
    "jin = {\"nuts\":'sack',\"balls\":\"2 of them\"}\n",
    "\n",
    "for key,val in jin.keys:\n",
    "    print(key,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 1000 patron accounts\n",
      "Sample of selected books:\n",
      "[(96, 25)]\n"
     ]
    }
   ],
   "source": [
    "fake = Faker()\n",
    "n_patrons = 1000\n",
    "print(f\"Building {n_patrons} patron accounts\")\n",
    "values = tuple(\n",
    "    zip([fake.name() for _ in range(n_patrons)],\n",
    "        [fake.address() for _ in range(n_patrons)],\n",
    "        [fake.unique.email() for _ in range(n_patrons)]))\n",
    "\n",
    "## CHAT GPT SUGGESTED ##\n",
    "\n",
    "# Function to simulate book selection\n",
    "# Function to simulate book selection\n",
    "def simulate_book_selection(persons, library):\n",
    "    selections = []\n",
    "    for person in persons:\n",
    "        if np.random.rand() < person['probability_of_reading']:\n",
    "            # Calculate weights based on hidden preferences\n",
    "            weights = np.array([person['hidden_preferences'].get(genre, 0) for genre in library['Genre']])\n",
    "\n",
    "            # Normalize weights to make them probabilities\n",
    "            weights /= weights.sum()\n",
    "\n",
    "            selected_book = np.random.choice(library['BookID'], p=weights)\n",
    "            selections.append((person['person_id'], selected_book))\n",
    "    return selections\n",
    "\n",
    "# Generate a list of book genres\n",
    "book_genres = ['Fiction', 'Mystery', 'Sci-Fi', 'Romance', 'Non-Fiction', 'Fantasy']\n",
    "\n",
    "# Create a library of books with genres\n",
    "num_books = 1000\n",
    "library = pd.DataFrame({\n",
    "    'BookID': np.arange(1, num_books + 1),\n",
    "    'Genre': np.random.choice(book_genres, size=num_books)\n",
    "})\n",
    "\n",
    "# Create a list of persons with assigned genres and hidden preferences\n",
    "num_persons = 100\n",
    "persons = []\n",
    "for person_id in range(1, num_persons + 1):\n",
    "    assigned_genres = np.random.choice(book_genres, size=np.random.randint(1, len(book_genres) + 1), replace=False)\n",
    "\n",
    "    # Generate hidden preferences for each person\n",
    "    hidden_preferences = {genre: np.random.rand() for genre in book_genres}\n",
    "\n",
    "    probability_of_reading = 0.01  # Very low probability of reading for each person\n",
    "    persons.append({\n",
    "        'person_id': person_id,\n",
    "        'assigned_genres': assigned_genres,\n",
    "        'hidden_preferences': hidden_preferences,\n",
    "        'probability_of_reading': probability_of_reading\n",
    "    })\n",
    "\n",
    "# Simulate book selections for 100,000 iterations\n",
    "num_iterations = 100000\n",
    "selected_books = simulate_book_selection(persons, library)\n",
    "\n",
    "# Display a sample of the selected books\n",
    "print(\"Sample of selected books:\")\n",
    "print(selected_books[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.4583333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Morri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Morri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text and remove stop words\n",
    "    tokens = [word.lower() for word in word_tokenize(text) if word.isalnum() and word.lower() not in stopwords.words('english')]\n",
    "    return set(tokens)\n",
    "\n",
    "def weighted_jaccard_similarity(book1, book2):\n",
    "    # Define weights for each text field\n",
    "    weights = {'title': 3, 'authors': 2, 'publisher': 1.5, 'description': 1, 'genres': 1}\n",
    "\n",
    "    # Preprocess each text field\n",
    "    preprocessed_fields1 = {field: preprocess_text(book1[field]) for field in weights.keys()}\n",
    "    preprocessed_fields2 = {field: preprocess_text(book2[field]) for field in weights.keys()}\n",
    "\n",
    "    # Calculate weighted Jaccard similarity for each text field\n",
    "    similarities = [(field, weights[field] * len(preprocessed_fields1[field].intersection(preprocessed_fields2[field])) /\n",
    "                     len(preprocessed_fields1[field].union(preprocessed_fields2[field])))\n",
    "                    for field in weights.keys()]\n",
    "\n",
    "    # Combine individual similarities using the assigned weights\n",
    "    overall_similarity = sum(similarity for _, similarity in similarities)\n",
    "\n",
    "    return overall_similarity\n",
    "\n",
    "# Example usage:\n",
    "book1 = {\n",
    "    'title': 'The Catcher in the Rye',\n",
    "    'authors': 'J.D. Salinger',\n",
    "    'publisher': 'Little, Brown and Company',\n",
    "    'publish_year': 1951,\n",
    "    'description': 'A classic novel about a teenage boy',\n",
    "    'genres': 'Fiction, Coming-of-age'\n",
    "}\n",
    "\n",
    "book2 = {\n",
    "    'title': 'To Kill a Mockingbird',\n",
    "    'authors': 'Harper Lee',\n",
    "    'publisher': 'J.B. Lippincott & Co.',\n",
    "    'publish_year': 1960,\n",
    "    'description': 'A novel set in the American South during the 1930s',\n",
    "    'genres': 'Fiction, Southern Gothic'\n",
    "}\n",
    "\n",
    "similarity_score = weighted_jaccard_similarity(book1, book2)\n",
    "print(f\"Similarity Score: {similarity_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating bookshelves...\n",
      "Generated 363 bookshelves containing 40000 books\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import populate_books\n",
    "\n",
    "books = populate_books.read_books_data()\n",
    "tfidf = TfidfVectorizer(analyzer='word',\n",
    "                      ngram_range=(1, 10),\n",
    "                      stop_words = 'english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = tfidf.fit_transform(books[\"Title\"].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<26737x341661 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 458900 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.33 GiB for an array with shape (26737, 26737) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\library_project\\library_project\\initialization\\populate_chechout.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/dbsys/library-project/library_project/library_project/initialization/populate_chechout.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m similarity_matrix \u001b[39m=\u001b[39m cosine_similarity(vecs, vecs)\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    215\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1586\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1583\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1584\u001b[0m     Y_normalized \u001b[39m=\u001b[39m normalize(Y, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m-> 1586\u001b[0m K \u001b[39m=\u001b[39m safe_sparse_dot(X_normalized, Y_normalized\u001b[39m.\u001b[39;49mT, dense_output\u001b[39m=\u001b[39;49mdense_output)\n\u001b[0;32m   1588\u001b[0m \u001b[39mreturn\u001b[39;00m K\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\.venv\\Lib\\site-packages\\sklearn\\utils\\extmath.py:200\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    192\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39m b\n\u001b[0;32m    194\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    195\u001b[0m     sparse\u001b[39m.\u001b[39missparse(a)\n\u001b[0;32m    196\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[0;32m    197\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[0;32m    198\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    199\u001b[0m ):\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39;49mtoarray()\n\u001b[0;32m    201\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\.venv\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1050\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m order \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1049\u001b[0m     order \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap(\u001b[39m'\u001b[39m\u001b[39mcf\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 1050\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_toarray_args(order, out)\n\u001b[0;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mc_contiguous \u001b[39mor\u001b[39;00m out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mf_contiguous):\n\u001b[0;32m   1052\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOutput array must be C or F contiguous\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\.venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:1267\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m   1266\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1267\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.33 GiB for an array with shape (26737, 26737) and data type float64"
     ]
    }
   ],
   "source": [
    "similarity_matrix = cosine_similarity(vecs, vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\library_project\\library_project\\initialization\\populate_chechout.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/dbsys/library-project/library_project/library_project/initialization/populate_chechout.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m doc2vec\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/dbsys/library-project/library_project/library_project/initialization/populate_chechout.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m doc2vec\u001b[39m.\u001b[39;49mDoc2Vec(books[\u001b[39m\"\u001b[39;49m\u001b[39mTitle\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mdrop_duplicates(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/dbsys/library-project/library_project/library_project/initialization/populate_chechout.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                         vector_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/dbsys/library-project/library_project/library_project/initialization/populate_chechout.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                         window\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/dbsys/library-project/library_project/library_project/initialization/populate_chechout.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                         min_count\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/dbsys/library-project/library_project/library_project/initialization/populate_chechout.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                         workers\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Morri/Documents/Notebooks/dbsys/library-project/library_project/library_project/initialization/populate_chechout.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                         dm\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m) \u001b[39m# PV-DBOW\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\.venv\\Lib\\site-packages\\gensim\\models\\doc2vec.py:296\u001b[0m, in \u001b[0;36mDoc2Vec.__init__\u001b[1;34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, shrink_windows, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39m# EXPERIMENTAL lockf feature; create minimal no-op lockf arrays (1 element of 1.0)\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m# advanced users should directly resize/adjust as desired after any vocab growth\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdv\u001b[39m.\u001b[39mvectors_lockf \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mREAL)  \u001b[39m# 0.0 values suppress word-backprop-updates; 1.0 allows\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[39msuper\u001b[39;49m(Doc2Vec, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    297\u001b[0m     sentences\u001b[39m=\u001b[39;49mcorpus_iterable,\n\u001b[0;32m    298\u001b[0m     corpus_file\u001b[39m=\u001b[39;49mcorpus_file,\n\u001b[0;32m    299\u001b[0m     vector_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvector_size,\n\u001b[0;32m    300\u001b[0m     sg\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m \u001b[39m+\u001b[39;49m dm) \u001b[39m%\u001b[39;49m \u001b[39m2\u001b[39;49m,\n\u001b[0;32m    301\u001b[0m     null_word\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdm_concat,\n\u001b[0;32m    302\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    303\u001b[0m     window\u001b[39m=\u001b[39;49mwindow,\n\u001b[0;32m    304\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m    305\u001b[0m     shrink_windows\u001b[39m=\u001b[39;49mshrink_windows,\n\u001b[0;32m    306\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    307\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\.venv\\Lib\\site-packages\\gensim\\models\\word2vec.py:429\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[39mif\u001b[39;00m corpus_iterable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m corpus_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m(epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m--> 429\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_vocab(corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, trim_rule\u001b[39m=\u001b[39;49mtrim_rule)\n\u001b[0;32m    430\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain(\n\u001b[0;32m    431\u001b[0m         corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, total_examples\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count,\n\u001b[0;32m    432\u001b[0m         total_words\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_total_words, epochs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs, start_alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha,\n\u001b[0;32m    433\u001b[0m         end_alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha, compute_loss\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss, callbacks\u001b[39m=\u001b[39mcallbacks)\n\u001b[0;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\.venv\\Lib\\site-packages\\gensim\\models\\doc2vec.py:882\u001b[0m, in \u001b[0;36mDoc2Vec.build_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_vocab\u001b[39m(\n\u001b[0;32m    842\u001b[0m         \u001b[39mself\u001b[39m, corpus_iterable\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, corpus_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, update\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, progress_per\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m,\n\u001b[0;32m    843\u001b[0m         keep_raw_vocab\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, trim_rule\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    844\u001b[0m     ):\n\u001b[0;32m    845\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build vocabulary from a sequence of documents (can be a once-only generator stream).\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \n\u001b[0;32m    847\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    880\u001b[0m \n\u001b[0;32m    881\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m     total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_vocab(\n\u001b[0;32m    883\u001b[0m         corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file,\n\u001b[0;32m    884\u001b[0m         progress_per\u001b[39m=\u001b[39;49mprogress_per, trim_rule\u001b[39m=\u001b[39;49mtrim_rule,\n\u001b[0;32m    885\u001b[0m     )\n\u001b[0;32m    886\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count \u001b[39m=\u001b[39m corpus_count\n\u001b[0;32m    887\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_total_words \u001b[39m=\u001b[39m total_words\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\.venv\\Lib\\site-packages\\gensim\\models\\doc2vec.py:1054\u001b[0m, in \u001b[0;36mDoc2Vec.scan_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m corpus_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1052\u001b[0m     corpus_iterable \u001b[39m=\u001b[39m TaggedLineDocument(corpus_file)\n\u001b[1;32m-> 1054\u001b[0m total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scan_vocab(corpus_iterable, progress_per, trim_rule)\n\u001b[0;32m   1056\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m   1057\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcollected \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m word types and \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m unique tags from a corpus of \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m examples and \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m words\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1058\u001b[0m     \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_vocab), \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdv), corpus_count, total_words,\n\u001b[0;32m   1059\u001b[0m )\n\u001b[0;32m   1061\u001b[0m \u001b[39mreturn\u001b[39;00m total_words, corpus_count\n",
      "File \u001b[1;32mc:\\Users\\Morri\\Documents\\Notebooks\\dbsys\\library-project\\.venv\\Lib\\site-packages\\gensim\\models\\doc2vec.py:956\u001b[0m, in \u001b[0;36mDoc2Vec._scan_vocab\u001b[1;34m(self, corpus_iterable, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[39mfor\u001b[39;00m document_no, document \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(corpus_iterable):\n\u001b[0;32m    955\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m checked_string_types:\n\u001b[1;32m--> 956\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(document\u001b[39m.\u001b[39;49mwords, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    957\u001b[0m             logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    958\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mEach \u001b[39m\u001b[39m'\u001b[39m\u001b[39mwords\u001b[39m\u001b[39m'\u001b[39m\u001b[39m should be a list of words (usually unicode strings). \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mFirst \u001b[39m\u001b[39m'\u001b[39m\u001b[39mwords\u001b[39m\u001b[39m'\u001b[39m\u001b[39m here is instead plain \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    960\u001b[0m                 \u001b[39mtype\u001b[39m(document\u001b[39m.\u001b[39mwords),\n\u001b[0;32m    961\u001b[0m             )\n\u001b[0;32m    962\u001b[0m         checked_string_types \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "from gensim.models import doc2vec\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "\n",
    "\n",
    "titles = preprocess_documents(books[\"Title\"].drop_duplicates(),)\n",
    "model = doc2vec.Doc2Vec(books[\"Title\"].drop_duplicates(),\n",
    "                        vector_size=100,\n",
    "                        window=5,\n",
    "                        min_count=1,\n",
    "                        workers=4,\n",
    "                        dm=0) # PV-DBOW"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
